{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1df617c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf4c40a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7aa43da",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatGroq(model=\"openai/gpt-oss-20b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eee19ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Gen‑AI (Generative AI) – A Quick Overview\n",
      "\n",
      "| **Aspect** | **What It Is** | **Why It Matters** |\n",
      "|------------|----------------|--------------------|\n",
      "| **Definition** | AI systems that *create* new content—text, images, audio, video, code, or even 3D models—rather than just *classify* or *regress* existing data. | It turns data into fresh, useful outputs that can be used for design, content creation, research, and more. |\n",
      "| **Core Technology** | Large‑scale neural networks (most commonly transformers) trained on vast datasets, learning statistical patterns and relationships. | These models can “imagine” plausible continuations or transformations, mimicking human creativity at scale. |\n",
      "| **Key Models** | GPT‑4, GPT‑5, Claude, LLaMA, Stable Diffusion, DALL‑E, Midjourney, AudioLM, etc. | Each model is tuned to a specific modality (text, image, audio) or multimodal tasks. |\n",
      "| **Training Process** | 1. **Data Collection** – scrape web, books, code, images, etc. <br>2. **Pre‑training** – learn general patterns by predicting missing tokens/patches. <br>3. **Fine‑tuning / Prompt Engineering** – adapt to specific tasks or styles. | The more diverse and high‑quality the data, the better the model generalizes. |\n",
      "| **Typical Tasks** | • Text generation (stories, summaries, code, dialogue)<br>• Image synthesis (illustrations, style transfer)<br>• Audio generation (music, speech synthesis)<br>• Video generation (short clips, animation)<br>• Multimodal translation (image captioning, text‑to‑image)<br>• Code generation & debugging | These tasks span creative arts, business productivity, scientific discovery, and accessibility. |\n",
      "| **How It Works (Simplified)** | 1. **Tokenization** – split input into tokens (words, sub‑words, pixels, audio samples). <br>2. **Embedding & Attention** – map tokens to vectors; self‑attention layers capture relationships. <br>3. **Decoding** – generate next token(s) iteratively (greedy, beam, nucleus sampling). | The attention mechanism lets the model weigh context from the entire prompt, producing coherent, context‑aware outputs. |\n",
      "| **Common Challenges** | • **Hallucinations** – fabricating facts or details.<br>• **Bias & Fairness** – reproducing harmful stereotypes present in training data.<br>• **Copyright & Attribution** – generating content that closely mimics copyrighted works.<br>• **Energy & Compute** – training large models is resource‑intensive.<br>• **Safety & Misuse** – potential for disinformation, deepfakes, or automated hacking. | Ongoing research focuses on mitigation techniques (e.g., RLHF, bias‑aware training, watermarking). |\n",
      "| **Ethical & Societal Impact** | • Democratizes content creation (writers, artists, developers).<br>• Raises questions about authorship and originality.<br>• Potential job displacement vs. new job creation. | Responsible deployment requires transparency, user control, and clear attribution. |\n",
      "| **Future Directions** | • **Multimodal & Embodied AI** – unified models that handle text, vision, audio, and robotics.<br>• **Efficient Models** – pruning, quantization, and sparse attention for edge deployment.<br>• **Explainability & Control** – tools for users to steer outputs and audit decisions.<br>• **Regulation & Governance** – frameworks for safe, fair, and accountable use. | These trends aim to make generative AI more powerful, accessible, and trustworthy. |\n",
      "\n",
      "---\n",
      "\n",
      "### Quick Takeaway\n",
      "\n",
      "Generative AI is a branch of artificial intelligence that *creates* new content by learning patterns from large datasets. Powered mainly by transformer‑based neural networks, it can produce text, images, audio, video, code, and more. Its applications are vast—from automating routine writing to aiding scientific research—but it also presents technical, ethical, and societal challenges that the community is actively addressing.\n"
     ]
    }
   ],
   "source": [
    "res=llm.invoke(\"Explain Gen-AI\")\n",
    "print(res.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
